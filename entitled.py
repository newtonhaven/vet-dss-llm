# -*- coding: utf-8 -*-
"""Entitled.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Iyac5p0w0t_sLPmJwqWQJzaNmpOLQ9Bi
"""

# @title Installing Depencies
!pip install transformers accelerate bitsandbytes huggingface_hub torch
!pip uninstall -y tensorflow
!pip install tensorflow-cpu
!pip install faiss-cpu sentence-transformers
!pip install pdfplumber

import os
from huggingface_hub import login
from google.colab import userdata

hf_token = userdata.get('hfToken')
login(token=hf_token)
os.environ["HF_HOME"] = str(hf_token)

# @title model installation
from huggingface_hub import snapshot_download
from pathlib import Path

mistral_models_path = Path.home().joinpath('mistral_models', '7B-Instruct-v0.3')
mistral_models_path.mkdir(parents=True, exist_ok=True)

snapshot_download(repo_id="mistralai/Mistral-7B-Instruct-v0.3", allow_patterns=["params.json", "consolidated.safetensors", "tokenizer.model.v3"], local_dir=mistral_models_path)

#@title loading model: Mistral 7B v0.3
from transformers import AutoModelForCausalLM, AutoTokenizer
import torch
import os

#quantization / not used on colab will fix later
# Load Mistral Model and Tokenizer
# model_name = "mistral-7b"
# tokenizer = AutoTokenizer.from_pretrained(model_name)
# model = AutoModelForCausalLM.from_pretrained(model_name, device_map="auto", torch_dtype=torch.float16)

model_path = "mistralai/Mistral-7B-Instruct-v0.3"
tokenizer = AutoTokenizer.from_pretrained(model_path)
model = AutoModelForCausalLM.from_pretrained(model_path)

input_text = "Explain what Retrieval-Augmented Generation is."
inputs = tokenizer(input_text, return_tensors="pt")
outputs = model.generate(**inputs)
print(tokenizer.decode(outputs[0]))

#@title Reading the Data
import pandas as pd

# Load to Excel files / THERE ARE 2 UNUSED DATASET
excel_path1 = '/content/dogDataSet.xlsx'
excel_path2 = '/content/dogThreatmentData.xlsx'

df1 = pd.read_excel(excel_path1)
df2 = pd.read_excel(excel_path2)

all_text_data = []
for df in [df1, df2]:
    for col in df.columns:
        all_text_data.extend(df[col].dropna().astype(str).tolist())  # Convert to string and handle NaN

import pdfplumber

# Load PDF files
pdf_path1 = '/content/CommonCatDiseases.pdf'
pdf_path2 = '/content/CommonDogDiseases.pdf'

for pdf_path in [pdf_path1, pdf_path2]:
    with pdfplumber.open(pdf_path) as pdf:
        for page in pdf.pages:
            all_text_data.append(page.extract_text())

# @title Using LM for Creating Vectoral DB
import faiss
import numpy as np
from sentence_transformers import SentenceTransformer

# Load an embedding model
embedding_model = SentenceTransformer("all-MiniLM-L6-v2")

# Creating embeddings for all text data
doc_embeddings = embedding_model.encode(all_text_data)

# Initializing FAISS
dimension = doc_embeddings.shape[1]
index = faiss.IndexFlatL2(dimension)
index.add(np.array(doc_embeddings))

# @title Pet Symptomps
from transformers import AutoModelForCausalLM, AutoTokenizer
import torch
from ipywidgets import widgets, VBox, HBox, Layout
from IPython.display import display

# Symptoms list
symptoms_list = [
    "Coughing", "Vomiting", "Lethargy", "Diarrhea", "Fever", "Bad breath",
    "Loss of Appetite", "Weight Loss", "Sneezing", "Itching", "Bloody stool",
    "Hair Loss", "Swelling", "Discharge", "Dehydration", "Weakness", "Gagging",
    "Pale Gums", "Excessive Drooling", "Aggression", "Anxiety", "Unable to jump up",
    "Seizures", "Panting", "Increased Thirst", "Increased Urination"
]

# Dynamically create checkboxes for symptoms
checkboxes = [widgets.Checkbox(value=False, description=symptom) for symptom in symptoms_list]

# Organize checkboxes in rows
symptom_rows = [HBox(checkboxes[i:i+3]) for i in range(0, len(checkboxes), 3)]  # 3 checkboxes per row

# Dynamically create checkboxes for symptoms
checkboxes = [widgets.Checkbox(value=False, description=symptom) for symptom in symptoms_list]

# Organize checkboxes in rows
symptom_rows = [HBox(checkboxes[i:i+3]) for i in range(0, len(checkboxes), 3)]  # 3 checkboxes per rowcheckboxes for symptoms
checkboxes = [widgets.Checkbox(value=False, description=symptom) for symptom in symptoms_list]

# Organize checkboxes in rows
symptom_rows = [HBox(checkboxes[i:i+3]) for i in range(0, len(checkboxes), 3)]  # 3 checkboxes per row

# Dropdown for pet type
pet_type = widgets.Dropdown(
    options=['Dog', 'Cat'],
    value='Dog',
    description='Pet:',
)

# Text input for age
age_input = widgets.Text(
    value='',
    placeholder='Enter age',
    description='Age:',
    layout=Layout(width='200px')
)


def validate_age():
    try:
        age = int(age_input.value)
        return age
    except ValueError:
        print("Age should be a NUMBER.")
        return None

# Submit button
submit_button = widgets.Button(
    description='Submit',
    button_style='success',
    tooltip='Submit form',
    icon='check',
)
exit_button = widgets.Button(
    description='Exit',
    button_style='Warning',
    tooltip='Exit form',
    icon='check',
)

def retrieve_context(query, k=5):
    query_embedding = embedding_model.encode([query])
    D, I = index.search(query_embedding, k)  # Retrieve top matches
    return [all_text_data[i] for i in I[0]]  # Return matching documents

# send data to mistral
def handle_submit(button):
    print("Generating Response May Take a While Please Be Patient\n********\n")

    validate_age()
    selected_pet = pet_type.value
    age = age_input.value
    if age == None:
        print("Age should be a NUMBER.")
        return
    selected_symptoms = [cb.description for cb in checkboxes if cb.value]

    user_query = f"Symptoms: {', '.join(selected_symptoms)} for a {selected_pet} aged {age}"
    relevant_context = "\n".join(retrieve_context(user_query))

    # Prepareing impact
    system_instructions = (
                        f"\"role\": \"system\", "
                        f"\"content\": You are a veterinarian assistant specialized in identifying potential illnesses in cats and dogs based on provided symptoms."
                        f"Your responses should be clear, concise, and actionable."
                        f"If symptoms indicate multiple possible illnesses, list them and specify the additional symptoms needed to confirm each illness, advising consultation with a veterinarian. "
                        f"Avoid speculation and alarming language."
                        f"Always prioritize the health and well-being of the pet in your advice."
                        f"If you don’t know the answer to a question, please don’t share false information."
                        f"Mention probability for illness if lower than 50%. If probability for illness is not higher than 30%, don’t mention."
                      )
    user_query = (
                        f"Animal: {selected_pet}\n"
                        f"Age: {age}\n"
                        f"Symptoms: {', '.join(selected_symptoms)}\n"
                        f"What illness does it have? \" #$#"
                )

    final_prompt = f"{system_instructions}\n\n{user_query}"

    # Tokenize input
    inputs = tokenizer(final_prompt, return_tensors="pt").to("cpu")

    # Generate response
    outputs = model.generate(**inputs, max_new_tokens=200, do_sample=True, temperature=0.7,eos_token_id=tokenizer.eos_token_id,pad_token_id=tokenizer.eos_token_id,)
    response = tokenizer.decode(outputs[0], skip_special_tokens=True)
    # output manupulation
    after_cut = response.split("#$#", 1)[1].strip()  # Get text after '#$#'  NEEDS MORE OUTPUT MANUPLATION, GONNA DO IT LATER

    # Displaying response
    print("Diagnosis and Advice:")
    print(response) # CHANGE AFTER CUT
    print("\n---\n")

submit_button.on_click(handle_submit)

# Display the whole thing
form = VBox([pet_type, age_input] + symptom_rows + [submit_button])
display(form)